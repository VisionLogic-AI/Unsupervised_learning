{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised Learning(Clustering Algorithms).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuh6tqX2rbEzgh2tEA+8HO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VisionLogic-AI/Unsupervised_learning/blob/master/Unsupervised_Learning(Clustering_Algorithms).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZv3bkyd2Dj",
        "colab_type": "text"
      },
      "source": [
        "#Clustering Algorithms \n",
        "Before we can perform clustering, we must perform dimensionality reduction using the most powerful unupervised learning algorithms at our disposal.\n",
        "\n",
        "In this notebook, we are going to use the MNIST dataset as an example dataset to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijZJnzxrdqdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "n_components= 784\n",
        "whiten= False\n",
        "random_state= 2018\n",
        "\n",
        "pca= PCA(n_components= n_components, whiten= whiten, random_state= random_state)\n",
        "\n",
        "x_train_pca= pca.fit_transform(x_train)   #training set from our previous notebook \n",
        "x_train_pca= pd.DataFrame(data= x_train_pca, index= x_train.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPDlEi2CezVN",
        "colab_type": "text"
      },
      "source": [
        "Although we did not reduce the dimensiionality, we will designate the number of principal components we will use during the clustering stage, effectively reducing the dimensionality.<br>\n",
        "In other worda, we will apply our chosen principal components during the clustering stage of the ML pipleine.\n",
        "\n",
        "Now leys move on to clustering. Clustering has three main algorithms:<br>\n",
        "- k means \n",
        "- hierarchical clustering\n",
        "- DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMp77ZJcfie8",
        "colab_type": "text"
      },
      "source": [
        "#K-Means Clustering\n",
        "The objectve of clustering is to id distinct group in a dataset such that the observations within a group are similar to each other but different from observations in other groups.<br>\n",
        "\n",
        "***In k-means clustering, we specifiy the number of desired clusters k, and the algorithm will assign each observation to exaclty one of these k-clusters.***<br>\n",
        "*The algorithm optimizes the groups by minimizing the \"within the cluster variation\"...also known as intertia, such that the su of the within the cluster variations across all k clusters is as small as possible.\n",
        "\n",
        "Clustering Steps:<br>\n",
        "1. We need to set the umber of clusters we would like **(n_clusters)**\n",
        "2. the number of initializations the algorithm will run to reassign observations in order to minimize inertia**(max_iter)**\n",
        "3. the tolerance to declare convergence**(tol)**\n",
        "\n",
        "In this example, we will keep the default values for the number of initializations to (10), maximum number of iterations (300), and tolerance (0.0001)<br>\n",
        "*Also, for now we will use the first 100 principal components from PCA as the *cutoff*.\n",
        "\n",
        "To test how the number of clusters we designate affects the inertia measure, lets run k means for cluster sizes 2 through 20 and record the inertia for each.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJQhSE2eh8Ky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#k means - Inertia as the number of clusters varies\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 10\n",
        "n_init= 10\n",
        "max_iter= 300\n",
        "tol= 0.0001\n",
        "random_state = 2018\n",
        "n_jobs= 2\n",
        "\n",
        "kmeans_inertia= pd.DataFrame(data= [], index= range(2,21), columns= ['intertia'])   #range between 2-20\n",
        "\n",
        "for n_clusters in range(2,21):\n",
        "  kmeans= KMeans(n_clusters= n_clusters, n_init= n_init, max_iter= max_iter,\n",
        "                 tol= tol, random_state= random_state, n_jobs= n_jobs)\n",
        "  cutoff= 99\n",
        "  kmeans.fit(x_train_pca.loc[:,0: cutoff])\n",
        "  kmeans_inertia.loc[n_clusters]= kmeans.inertia_\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7wX_DYHpzJg",
        "colab_type": "text"
      },
      "source": [
        "The figure shows the inertia decrease as the number of clusters increases...this makes sense.<br>\n",
        "The more clusters we have, the greater the homogeneity among observations within each cluster.<br>\n",
        "However, fewer clusters are easier to work with than more, so finding the right number of clusters to generate is an important consideration when running k-means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ7EPno1qUVQ",
        "colab_type": "text"
      },
      "source": [
        "#Evaluating the Clustering Results\n",
        "To demonstrate how k-means works and how increasing the number of clusters results in more homogenpous clusters, lets define a function that can analyze the results.<br>\n",
        "*The cluster assignments generatedby the clustering algorithm will be stored in a pandas dataframe called clusterDF.\n",
        "\n",
        "Lets count the number of observations within each cluster and store these in  a Pandas dataframe called countybycluster\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYr_56ALjHZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyze_cluster(clusterDF, labelsDF):\n",
        "  countbycluster= pd.DataFrame(data= clusterDF['cluster'].value_counts())\n",
        "  countbycluster.reset_index(inplace= True, drop= False)\n",
        "  countbycluster.columns= ['cluster', 'cluster_count']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhy6TXTvjsFV",
        "colab_type": "text"
      },
      "source": [
        "Next lets join the clusterDF with the true labels array, which we will call labelsDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3WEUQIOjzO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds= pd.concat([labelsDF, clusterDF], axis= 1)\n",
        "preds.columns= ['truelabel', 'cluster']\n",
        "\n",
        "#lets also count the number of observations for each true label in the training set \n",
        "#this won't change but is good for us to know\n",
        "\n",
        "countbylabel= pd.DataFrame(data= preds.groupby('truelabel').count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayns-d6zktfC",
        "colab_type": "text"
      },
      "source": [
        "For example, if a given cluster has three thousand observations:\n",
        "  - two thousand may represent the number 2\n",
        "  - five hundred may represent the number 1\n",
        "  - three nundred may represent the number 0\n",
        "  - the remaining two hundred may represent the number 9\n",
        "\n",
        "Once we calculate these, we will store the count for the most frequently occuring number for each cluster.\n",
        "\n",
        "Lets use this example above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRrUqEwulTTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "countmostfreq= pd.DataFrame(data = preds.groupby('cluster').agg(lambdax:x.value_counts().iloc[0]))\n",
        "countmostfreq.reset_index(inplace= True, drop= False)\n",
        "countmostfreq.columns= ['cluster', 'CountMostFrequent']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8VmHZxqlzbI",
        "colab_type": "text"
      },
      "source": [
        "Finally, we will judge the success of each clustering run based on how tightly grouped the observations are within each cluster.<br>\n",
        "For example, the example above has two thousand observations that have the same label out of a total of three thousand observations in the cluster.\n",
        "\n",
        "This cluster is not great since we ideally want to group similar observations together in the same cluster and exclude dissimilar ones.\n",
        "\n",
        "Lets define the overall accuracy of the clustering as the sum of the counts of the most frequently occuring observations across all the clusters divided by the total number of observations in the training set. (ex. 50,000)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPL7S45_mpKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code\n",
        "accuracyDF= countmostfreq.merge(countbycluster, left_on= 'cluster', right_on= 'cluster')\n",
        "overall_accuracy= accuracyDF.countmostfreq.sum()/ accuracyDF.clusterCount.sum()\n",
        "#we cal also access the accuracy by cluster\n",
        "accuracy_by_label= accuracyDF.countmostfreq/ accuracyDF.clusterCount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1Egk9jDnS67",
        "colab_type": "text"
      },
      "source": [
        "#K-Means Accuracy\n",
        "Lets now perform these expirements we did earlier, but instead of calculating the inertia, we will instead focus on the overall homogeneity of the digits dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSEech9u4BdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#k-means Accuracy as the number of clusters varies\n",
        "\n",
        "n_clusters= 5\n",
        "n_init= 10\n",
        "max_iter= 300\n",
        "tol= 0.0001\n",
        "random_state= 2018\n",
        "n_jobs= 2\n",
        "\n",
        "kmeans_inertia= pd.DataFrame(data= [], index= range(2,21), columns= ['inertia'])\n",
        "overall_accuracy_kmeansDF= pd.DataFrame(data= [], index= range(2,21), columns= ['overall_accuracy'])\n",
        "\n",
        "for n_clusters in range(2,21):\n",
        "  kmeans= KMeans(n_clusters= n_clusters, n_init= n_init, max_iter= max_iter, \n",
        "                 tol= tol, random_state= random_state, n_jobs= n_jobs)\n",
        "  cutoff = 99\n",
        "  kmeans.fit(x_train_pca.loc[:, 0: cutoof])\n",
        "  kmeans_inertia.loc[n_clusters]= kmeans_inertia_\n",
        "\n",
        "  x_train_kmeans_clustered= kmeans.predict(x_train_pca.loc[:, 0: cutoof])\n",
        "  x_train_kmeans_clustered= pd.DataFrame(data= x_train_kmeans_clustered, index= x_train.index, columns= ['cluster'])\n",
        "  \n",
        "  countybycluster_kmeans, countybylabel_kmeans, countmostfreq_kmeans, accuracydf_kmeans, overallaccuracy_kmeans, accuracybylabels_kmeans= analyze_cluster(\n",
        "      x_train_kmeans_clustered, y_train)\n",
        "  \n",
        "  overall_accuracy_kmeansDF.loc[n_clusters]= overallaccuracy_kmeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx7WHZgv6OMI",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the accuracy improves as the number of clusters increases.<br>\n",
        "In other words, clusters become more homogeneous as we increase the number of clusters because each cluster becomes smaller and more tightly formed.\n",
        "\n",
        "Accuracy by cluster may vary quite a bit, with some clusters exibiting a high degree of homogeneity and others exibiting less.\n",
        "\n",
        "For example, in some clusters, over 90% of the images have the same digit, in other clusters, less than 50% of the images have the same digit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk-HDj9j7LyG",
        "colab_type": "text"
      },
      "source": [
        "#K-Means and Number of Principal Components\n",
        "Let prform another experiment but this time we will assess how varying the number of principal components we use in the clustering algorithm impacts the homogeneity of the clusters (defind as accuracy)\n",
        "\n",
        "In the experimnets earlier, we used one hundred principal components, derived from the normal PCA.<br>\n",
        "(Recall that the original number of dimensions within the dataset is 784).\n",
        "\n",
        "Now, if PCA does a good job of capturing the salient information or structure within the data, as compactly as possible, the clustering alogorithm will have an easy time grouping similar images together, regardless of whether we use 10-50 components or or hundreds of them.\n",
        "\n",
        "Lets pass along 10, 50, 100, 200, 300, 400, 500, 600, and 700 components out of the 784 total observations and then gauge the accuracy of each clustering experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3eQLChs-qAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#k-means (Accuracy as the number of components varies)\n",
        "n_cluster 20\n",
        "n_init= 10\n",
        "max_iter= 300\n",
        "tol= 0.0001\n",
        "random_state= 2018\n",
        "n_jobs= 2\n",
        "\n",
        "kmeans_inertia= pd.DataFrame(data= [], index= [9, 49, 99, 199, 299, 399, 499,\n",
        "                                               599, 699, 784], columns= ['inertia'])\n",
        "overallaccuracy_kmeansDF= pd.DataFrame(data= [], index[9, 49, 99, 199, 299, 399, \n",
        "                                                       499, 599, 699, 784], columns= ['overall_accuracy'])\n",
        "\n",
        "for cutoffnumber in [9, 49, 99, 199, 299, 399, 499, 599, 699, 784]:\n",
        "  kmeans= KMeans(n_clusters= n_clusters, n_init= n_init, max_iter= max_iter,\n",
        "                 tol= tol, random_state= random_state, n_jobs= n_jobs)\n",
        "  cutoff= cutoffnumber\n",
        "  kmeans.fit(x_train_pca.loc[:, 0:cutoff])\n",
        "  kmeans_inertia.loc[cutoff]= kmeans.inertia_\n",
        "\n",
        "x_train_kmeansclustered= kmeans.predict(x_train_pca.loc[:, 0: cutoff])\n",
        "x_train_kmeansclustered= pd.DataFrame(data= x_train_kmeansclustered, index= x_train.index, columns= ['cluster'])\n",
        "\n",
        "countybycluster_kmeans, countbylabels_kmeans, countmostfreq, accuracydf_kmeans, overallaccuracy_kmeans, accuracybylabels_kmeans= analyzecluster(\n",
        "    x_train_kmeansclustered, y_train)\n",
        "\n",
        "overallaccuracy_kmeansDF.loc[cutoff]= overallaccuracy_kmeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klSVdXpLR8q8",
        "colab_type": "text"
      },
      "source": [
        "As we can see this plot supports out hypothesis.<br>\n",
        "As the number of principal components varies from 10-784, the clustering accuracy remains stable and consistent around 70%.<br>\n",
        "\n",
        "This is one reason why clustering should be performed on dimensionality reduced datasets- the clustering algorithms generally perform better, both in terms of time and clustering accuracy on dimensionality reduced datasets.\n",
        "\n",
        "Now imagine if our dataset has hundreds of thousands or millions of dimensions, this is really where using dimensionality reduction would shine. (Big Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp67tsZSSuSh",
        "colab_type": "text"
      },
      "source": [
        "#K_Means on the Original Dataset\n",
        "To make this point clearer, lets  perform clustering on the original dataset and measure how varying the number of dimensions we pass into the clustering algorithm affects clustering accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKpBewOPTCmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#k-means  - Accuracy as the number of components varies\n",
        "#On the \"original (non dimensioanlity reduced dataset)\n",
        "\n",
        "n_clusters = 20\n",
        "n_init= 10\n",
        "max_iter= 300\n",
        "tol= 0.0001\n",
        "random_state= 2018\n",
        "n_jobs= 2\n",
        "\n",
        "kmeans_inertia= pd.DataFrame(data= [], index= [9,49,99,199,299,399,\n",
        "                                               499,599,699,784], columns = ['inertia'])\n",
        "overallaccuracy_kmeansDF= pd.DataFrame(data= [], index= [9, 49, 99, 199, 299, 399,\n",
        "                                                         499, 599, 699, 784], columns= ['overall_accuracy'])\n",
        "\n",
        "for cutoffnumber in [9, 49, 99, 199, 299, 399, 499, 599, 699, 784]:\n",
        "  kmeans= KMeans(n_clusters= n_clusters, n_init= n_init, max_iter= max_iter, \n",
        "                 tol= tol, random_state= random_state, n_jobs= n_jobs)\n",
        "  cutoff = cutoffnumber\n",
        "  kmeans.fit(x_train.loc[:, 0: cutoff])\n",
        "  kmeans_inertia.loc[cutoff]= kmeans.inertia_\n",
        "\n",
        "x_train_kmeansclustered= kmeans.predict(x_train.loc[:, 0:cutoff])\n",
        "x_train_kmeansclustered= pd.DataFrame(data= x_train_kmeansclustered, index = x_train.index, columns= ['cluster'])\n",
        "\n",
        "countbycluster_kmeans, countbylabel_kmeans, countmostfreq_kmeans, accuracydf_kmeans, overallaccuracy_kmeans, accuracybylabel_kmeans= analyzecluster(\n",
        "    x_train_kmeansclustered, y_train)\n",
        "\n",
        "overallaccuracy_kmeansDF.loc[cutoff]= overallaccuracy_kmeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J3ATBUpVH-5",
        "colab_type": "text"
      },
      "source": [
        "This is going to plot the clustering accuracy at the different original dimensions.\n",
        "\n",
        "As the plot shows, the clustering accuracy is very poor at lower dimensions BUT improves to nearly 70% only only as the number of dimensions climbs to six hundred dimensions.\n",
        "\n",
        "*In the PCA case, clustering accuracy was roughly 70% even at 10 dimensions, demonstrating the power of dimensionality reduction to densely capture salient informationn in the original dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7L9ca1mW_vP",
        "colab_type": "text"
      },
      "source": [
        "#Hierarchical Clustering\n",
        "This approach does not require us to pre-commit to a particular number of clusters. <br>\n",
        "Instead, we can choose how many clusters we would like after hierarchical clustering has finished running.\n",
        "\n",
        "This algorithm will build what's called a dendrogram, which can be depicted as an upside down tree where the leaves are at the bottom and the branch is at the tree trunk is at the top.\n",
        "\n",
        "The leaves at the very are individual instances in the dataset. This algorithm then joins the leaves together = as we move vertically up the upside down tree- based on how similar they are to each other.<br>\n",
        "*The instances (or group of instances) that are most similar to each other are joined sooner, while the instances that are not as similar are joined later.\n",
        "\n",
        "With this iterative process, all of the instances are eventually linked together forming the \"single trunk fof the tree\".\n",
        "\n",
        "\n",
        "**This vertical depiction is very helpful seeing thatg we can view the dendrogram and determine where we want to cut the tree. The lower we, the more individual branches we are left with (more clusters).<br>\n",
        "If we want fewer clusters, we can cut higher on the dendrogram, closer to the single trunk at the very top of this upside down.\n",
        "<br>\n",
        "This is similar to choosing the number of k-clusters in the k-means clustering algorithm.\n",
        "\n",
        "**WE WILL EXPLORE A VERSION OF THIS ALGORITHM CALLED AGGLOMERATIVE HIERARCHICAL CLUSTERING ** (known as FastCluster when coding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwmJAJoEZ_ah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fastcluster\n",
        "from scipy.cluster.hierarchy import dendrogram, cophenet\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "cutoff= 100  #only using 100 components\n",
        "z= fastcluster.linkage_vector(x_train_pca.loc[:, 0: cutoff], method= 'ward', metric= 'euclidean')\n",
        "z_dataframe= pd.DataFrame(data= z, columns= ['cluster_one', 'cluster_two', 'distance', 'New_Cluster_Size'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G160g6wdbCgi",
        "colab_type": "text"
      },
      "source": [
        "The algorithm will return a maxtrix Z.<br>\n",
        "The algorithm treats each observation in our 50,000 mnist dataset as a single point cluster, and in each iteration of training, the algorithm will merge the two clusters that have the smallest distance between them.\n",
        "\n",
        "*Initially, the algorithm is just merging single point clusters together, but as it proceeds, it will merge multipoint clusters with either single point or multi-point clusters. <br>\n",
        "Eventually through this iterative process, the all the clusters are merge together, forming the trunk in the upside down tree. (dendrogram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m05iM_rfzEb",
        "colab_type": "text"
      },
      "source": [
        "#Evaluating the Clustering Results\n",
        "Now that we have the dendrogram in place, lets determine where to cut off of the dendrogram to make the number of clusters we desire.\n",
        "\n",
        "Now in order to compare this type of algorithm to the k-means clustering algorithm, lets cut the dendrogram to have exactly 20 clusters.<br>\n",
        "We will then use the clustering accuracy metric- defined in the k-means section- to judge how homogenous the hierarchical clustering clusters are.\n",
        "\n",
        "To create the clusters we desire the dendrogram, lets pull in the fcluster lib from scipy.<br>\n",
        "We need to specify the distance threshold of the dendrogram to determine how many \"distinct\" clusters we are left with.<br>\n",
        "*The larger  the distance threshold, the fewer clusters we will have.\n",
        "\n",
        "To get exactly 20 clusters, we need to experiment with the distance threshold.<br>\n",
        "*The fcluster lib will take our dendrogram and cut it with the distance threshold we specify. Each observation in the 50,000 observations MNIST digits dataset will get a clsuter label, adn we will store these in a pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blOMmY0yhcVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "distance_threshold= 160\n",
        "clusters= fcluster(z, distance_threshold, criterion= 'distance')\n",
        "x_train_hierclustered= pd.DataFrame(data= clusters, index= x_train_pca.index,\n",
        "                                    columns= ['cluster'])\n",
        "\n",
        "#lets verify that there are exaclty 20 distinct clusters, given our choice of distance threshold\n",
        "print('Number of distinct clusters: ', len(x_train_hierclustered['cluster'].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOQcmh6pjlhY",
        "colab_type": "text"
      },
      "source": [
        "As expected, this confirms 20 clusters.<br>\n",
        "Now lets evaluate the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxy96WI5jxgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "countbycluster_hierclust, countbylabel_hierclust, countmostfreq_hierclust, accuracydf_hierclust, overallallaccuracy_hierclust, accuracylabel_hierclust= analyze_cluster(\n",
        "    x_train_hierclustered, y_train)\n",
        "print(('Overall accuracy from hierarchical clustering: ', overallaccuracy_hierclust)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB4w2wvIkTxf",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the overall accuracy is approx. 77%...even better than the approx for k-means algorithm.\n",
        "\n",
        "Lets also assess the accuracy rate by cluster. <br> \n",
        "As we can see, some clusters have an accuracy score as high as 100& while other as low as 50%. This gives us an overall accuracy score of roughly 77%...not bad at all!\n",
        "\n",
        "Overall, the hierarchical clustering algorithm performs well on MNIST dataset. And this was all done without using ANY labels.\n",
        "\n",
        "This is how it would work on real-world examples:\n",
        "  - Step 1: Apply dimensionality reduction first (such as PCA)\n",
        "  - Step 2: perform clustering (such as hierarchical clustering)\n",
        "  - Step 3: Hand label a few points per cluster\n",
        "\n",
        "*So long as the clusters we homogenous enough (same kind or alike), the few hand labels we generated could be applied automatically to all of the other images in the cluster.\n",
        "\n",
        "All of a sudden, without much effort, we could have labeled all of the images in our 50,000 dataset with a near 77% accuracy. <br>\n",
        "\n",
        "*#THIS IS IMPRESSIVE AND HIGHLIGHTS THE POWER OF UNSUPERVISED LEARNING! **bold text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfCS90qIni5k",
        "colab_type": "text"
      },
      "source": [
        "#DBSCAN\n",
        "This final clustering algorithm groups based on the density of points.<br>\n",
        "In k-meand and herarchical clustering algorithms, all points had to be clustered in some way shape or form...***and outliers we poorly dealt with.***<br>\n",
        "\n",
        "**In DBSCAN, we can explicitly label points as outliers and avoid having tok cluster them in the first place....THIS IS VERY POWERFUL!!**\n",
        "\n",
        "Compared to other clustering algorithms, DBSCAN is much less prone to the distortion typically caused by outliers in the data.<br>\n",
        "(Also, like hierarchical clustering, and unlike k-means, we do not need to prespecify the number of clusters)\n",
        "\n",
        "All points that do not get grouped- either as the core border points of a cluster- are labeled as outliers.\n",
        "\n",
        "In this example, we are going to apply DBSCAN to the first 100 compenents within our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coGhhiUCpQtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DBSCAN\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "eps= 3\n",
        "min_samples = 5     #default value\n",
        "leaf_size= 30\n",
        "n_jobs = 4\n",
        "\n",
        "\n",
        "db= DBSCAN(eps = eps, min_samples = min_samples, leaf_size= leaf_size,\n",
        "           n_jobs= n_jobs)\n",
        "cutoff= 99     #100 components we are using (counting 0 as the number 1)\n",
        "x_train_pca_dbscan_clustered= db.fit_predict(x_train_pca.loc[:,0:cutoff])\n",
        "x_train_pca_dbscan_clustered= pd.DataFrame(data= x_train_pca_dbscan_clustered, index= x_train.index,\n",
        "                                           columns= ['cluster'])\n",
        "\n",
        "countbycluster_dbscan, countbylabel_dbscan, countmostfreq_dbscan, accuracyDF_dbscan, overallaccuracy_dbscan, accuracybylabel_dbscan= analze_cluster(\n",
        "    x_train_pca_dbscan, y_train)\n",
        "\n",
        "print('Overall Accuracy DBSCAN: ' overallaccuracy_dbscan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8Eebe0XtxFz",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the accuracy is very poor compared to k-means and hierarchical clustering.<br>\n",
        "We can fidget around with some of the parameters such as \"eps\" and \"min_samples\" to improve results, but it appears thay DBSCAN is poorly suited to cluster the observations for this particular dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLwF_f90vFMw",
        "colab_type": "text"
      },
      "source": [
        "#Conclusion\n",
        "In this notebook, we introduced three major clustering algorithms known as:\n",
        "  - k-means clustering\n",
        "  - hierachical clustering\n",
        "  - DBSCAN\n",
        "\n",
        "The first two algorithms performed the best, with hierachical clustering coming out on top."
      ]
    }
  ]
}