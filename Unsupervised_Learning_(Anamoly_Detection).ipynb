{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised  Learning (Anamoly Detection).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMBoNd2Aw3puby+8yYnjWiI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VisionLogic-AI/Unsupervised_learning/blob/master/Unsupervised_Learning_(Anamoly_Detection).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsns4rxwtadT",
        "colab_type": "text"
      },
      "source": [
        "#Credit Card Fruad Detection (Anomaly Detection)\n",
        "Lets build an applied machine learning solution using dimensionality reduction methods we created in our previous notebook.\n",
        "\n",
        "In this notebook, we will build a fraud detection system using unsupervised learning....no labels!\n",
        "\n",
        "In the real world, fraud often goes undiscoverd, and only the fraud that is caught provides labels for the dataset.\n",
        "Moreover, fraud patterns change over time, so \"supervised\" systems that are built using fraud labels become stale, capturing historical patterns of fraud but failing to adapt to newly emrging patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v35aHT_HvBeL",
        "colab_type": "text"
      },
      "source": [
        "#Prepare the Data\n",
        "Lets load the credit card transactions dataset, generate the features matrix and labels arrays, and then split the data into training and test sets.\n",
        "*We will not use the labels to perform anomaly detection, but we will use the labels to help evaluate the fraud detection systems we build."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXS_P6mMuGQB",
        "colab_type": "code",
        "outputId": "ecdde070-ae7e-40c8-8c4b-6c1b9130f4d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/aapatel09/handson-unsupervised-learning.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'handson-unsupervised-learning'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 459 (delta 14), reused 21 (delta 5), pack-reused 410\u001b[K\n",
            "Receiving objects: 100% (459/459), 93.79 MiB | 30.20 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7fLj8OJwCuX",
        "colab_type": "code",
        "outputId": "50db6915-2b1f-4754-c83e-f22e8544590b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#use the correct version of tensorflow\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpyswaoqwQSj",
        "colab_type": "code",
        "outputId": "9e60c7b7-52e3-4e62-d593-a0f13f13d583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!pip install keras\n",
        "!pip install xgboost"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWBcUxitxpCh",
        "colab_type": "code",
        "outputId": "0cd2e7cb-a6ec-45a6-85c1-e7d179aac1ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!pip install lightgbm\n",
        "!pip install fastcluster\n",
        "!pip install tslearn"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (0.15.1)\n",
            "Collecting fastcluster\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/9d/3d7525a4722ee4a11ad969762d1de53b6dac326b5ac1366221e06958e1d7/fastcluster-1.1.26-cp36-cp36m-manylinux1_x86_64.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from fastcluster) (1.18.5)\n",
            "Installing collected packages: fastcluster\n",
            "Successfully installed fastcluster-1.1.26\n",
            "Collecting tslearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/2c/e1bbc15e38e5fa8ffc5b402a3bd91fb1d3e44fc35e8e491eb540bc3aaa38/tslearn-0.4.0-cp36-cp36m-manylinux2010_x86_64.whl (770kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from tslearn) (0.48.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tslearn) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tslearn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from tslearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from tslearn) (0.15.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from tslearn) (0.29.19)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->tslearn) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->tslearn) (47.1.1)\n",
            "Installing collected packages: tslearn\n",
            "Successfully installed tslearn-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt5xLoNtwbL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ddfa395d-1d10-4eae-a708-5c47b1aaec05"
      },
      "source": [
        "#load dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, time\n",
        "import pickle, gzip\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import preprocessing as pp\n",
        "from scipy.stats import pearsonr\n",
        "from numpy.testing import assert_array_almost_equal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "#unsupervised learning algos\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.decomposition import SparsePCA, KernelPCA, TruncatedSVD\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from sklearn.random_projection import SparseRandomProjection\n",
        "from sklearn.manifold import Isomap, MDS, LocallyLinearEmbedding, TSNE\n",
        "from sklearn.decomposition import MiniBatchDictionaryLearning, FastICA\n",
        "\n",
        "\n",
        "\n",
        "file = '/content/handson-unsupervised-learning/datasets/credit_card_data/credit_card.csv'\n",
        "data= pd.read_csv(file)\n",
        "#datax= data.copy().drop(['Class'],axis= 1)\n",
        "#datay= data['Class'].copy()\n",
        "\n",
        "features_to_scale= data.columns\n",
        "sx= pp.StandardScaler(copy= True)\n",
        "data.loc[:, features_to_scale]= sx.fit_transform(data[features_to_scale])\n",
        "\n",
        "x_train, x_test, y_train, y_test= train_test_split(data,test_size= 0.33,\n",
        "                                                   random_state= 2018, stratify= datay)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2c8e135630f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mfeatures_to_scale\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0msx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_to_scale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures_to_scale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m x_train, x_test, y_train, y_test= train_test_split(data,test_size= 0.33,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    698\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'),\n\u001b[1;32m    699\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'oid sha256:76274b691b16a6c49d3f159c883398e03ccd6d1ee12d9d8ee38f4b4b98551a89'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8eKcgaR3kAo",
        "colab_type": "text"
      },
      "source": [
        "#Define Anomaly Score Function\n",
        "Next, we need to define a function that calculates how anomalous each transaction is. \n",
        "*The more anamolous the transaction is, the more likely is it to be fraudulent\n",
        "\n",
        "First, we will ned to rescale the data between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfNdtwVn4fup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function\n",
        "def anomalyscores(originaldf, reduceddf):\n",
        "  loss= np.sum((np.array(oringaldf)- np.array(reduceddf))**2, axis= 1)\n",
        "  loss= pd.Series(data= loss, index= originaldf.index)\n",
        "  loss= (loss-np.min(loss)/np.max(loss)- np.min(loss))\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37l6sG6R5PYW",
        "colab_type": "text"
      },
      "source": [
        "#Define Evaluation Metrics\n",
        "Although we will not use the fraud detection labels in order to build the unsupervised fraud detection system, we will use the labels to evaluate the unsupervised model we develop.\n",
        "*The labels will help us understand just how well these solutions are catching known patterns of fraud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He6tAfwg5sWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function\n",
        "def plot_results(truelabels, anomaly_scores, return_preds= False):\n",
        "  preds= pd.concat([truelabels, anomaly_scores], axis= 1)\n",
        "  preds.columns= ['truelabel', 'anomalyscore']\n",
        "  precison, recall, threshold= average_precision_score(preds['truelabel'], preds['anomalyscore'])\n",
        "  plt.step(recall, precision, color= 'k', alpha= 0.7,\n",
        "           where= 'post')\n",
        "  plt.fill_between(recall, precision, step= 'post', alpha= 0.3, color= 'k')\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.title('Precision Recall Curve: Average Precision= {0:02f}'.format(average_precision))\n",
        "  fpr, tpr, thresholds= roc_curve(preds['truelabel'], preds['anomalyscore'])\n",
        "  area_under_roc= auc(fpr, tpr)\n",
        "  plt.figure()\n",
        "  plt.plot(fpr, tpr, color= 'r', lw=2, label= \"ROC Curve\")\n",
        "  plt.plot([0,1], [0,1], color='k',lw=2, linestyle= '--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Receiver operating charateristic: Area Under Curve= {0:0.2f}'.format(area_under_roc))\n",
        "  plt.legend(loc= 'lower right')\n",
        "  plt.show()\n",
        "  if return_preds== True:\n",
        "    return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw9jeu0TzteF",
        "colab_type": "text"
      },
      "source": [
        "Teh fraud labels and the evaluatin metrics will help us assess just how good the unsupervised fraud detection systems are at catching known patterns of fraud- fraud that we have caught in the past and have labels for.\n",
        "\n",
        "However,we will not be able to assess how good the unsupervised detection systems are at catching unknown patterns of fraud.\n",
        "*In other words, there may be fraud in the dataset that is incorrectly labeled as not fraud because the financial company **never discovered it**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjwD3isC0gWv",
        "colab_type": "text"
      },
      "source": [
        "#Define Plotting Function\n",
        "We wil reuse the scatterplot function in the earlier notebook to display the separation of points the dimensioanlity reduction alogorithm achieves in just the first tw dimensions:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYflRvTL0zlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scatterPlot(xdf, ydf, algoname):\n",
        "  tempdf= pd.DataFrame(data= xdf.loc[:,0:1], index= xdf.index)\n",
        "  tempdf= pd.concat((tepdf, ydf), axis= 1, join= 'inner')\n",
        "  tempdf.columns= ['First Vector', 'Second Vector', 'Label']\n",
        "  sns.implot(x= 'First Vector', y= 'Second Vector', hue= 'Label', data= tempdf,  fit_reg= False)\n",
        "  ax= plt.gca()\n",
        "  ax.set_title('Separation of Observations Using'+algoname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qGlHXdO1rOC",
        "colab_type": "text"
      },
      "source": [
        "#Normal PCA Anomaly Detection\n",
        "We will now use PCA to learn the underlying structure of the credit card transactions dataset.\n",
        "Once we learn this structure, we will use the learned model to reconstruct the credit card transactions, and then calculate \"how different\" the reconstructed transactions are from the original transactions.\n",
        "*Those transactions that PCA does the poorest job of reconstructing are the most anomalous and most likely to be fraud. **bold text**\n",
        "\n",
        "Deeper Understanding:<br>\n",
        "***Anomaly detection  relies on reconstruction error. We want the reconstruction error fpr rare transaction- the ones that are most likely to be fraudulant- to be as high as possible and thereconstruction error for the rest to be as low as possible.***\n",
        "\n",
        "\n",
        "For PCA, tbhe reconstruction error will depend largely on the number of principal components we keep and use to reconstruct the original transactions.\n",
        "*The more principal components we keep, the better the PCA will be at learning the underlying structure of the original transactions.\n",
        "\n",
        "HOWEVER....THERE IS A BALANCE!\n",
        "\n",
        "If we keep topo many principal components, PCA may too easily reconstruct the original transactions. If we keep too few, PCA may not be able to reconstruct any of the original transactions well enough- not even the normal, non fraudulent transactions.\n",
        "\n",
        "Lets search for the right number of principal components to keep in order to build a good fraud detection system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1y0dCi6hp6",
        "colab_type": "text"
      },
      "source": [
        "#PCA Components Equal Number of Original Dimensions\n",
        "If we use PCA to generate the same number of principal components as the number of original features, we will be able to perform anomaly detection?\n",
        "\n",
        "The answer should be obvious.\n",
        "\n",
        "When the number of principal components equals the number of original dimensions, PCA captures nearly 100% of the variance/information in the data as it generates the principal components.\n",
        "Therefore, when PCA reconstructs transactions from the principal components, it will have too little reconstruction error for all the transactions, fraudulant or otherwise...in other words anomaly detection would be poor.\n",
        "\n",
        "To highlight this point, lets apply PCA to generate the same numbert of principal components as the number of original features (30 for our credit card transaction dataset). <br>\n",
        "*This is accomplished with the fit_transform function used in sklearn. To reconstruct trhe original transactions from the principal components we generate, we will use the inverse_transform function. **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1HwPPjI8KHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#30 principal components\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "n_components= 30\n",
        "whiten = False\n",
        "random_state= 2018\n",
        "\n",
        "pca= PCA(n_components=n_components, whiten= whiten, random_state= random_state)\n",
        "\n",
        "x_train_pca= pca.fit_transform(x_train)\n",
        "x_train_pca= pd.DataFrame(data= x_train_pca, index= x_train.index)\n",
        "x_train__pca_inverse= pca.inverse_transform(x_train_pca)\n",
        "x_train__pca_inverse= pd.DataFrame(data= x_train_pca_inverse, index= x_train.index)\n",
        "\n",
        "scatterPlot(x_train_pca, y_train, 'PCA')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knqRIvtg-OXZ",
        "colab_type": "text"
      },
      "source": [
        "Lets calculate the precision recall curve and ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLnJoFP9-UfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anomaly_scores_pca= anomaly_scores(x_train, x_train_pca_inverse)\n",
        "preds= plotResults(y_train, anomaly_scores_pca, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9U8dux__LQj",
        "colab_type": "text"
      },
      "source": [
        "With an average precison of 0.11, this is a poor fraud detection solution. It catches very little fraud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyfv3mhFt41x",
        "colab_type": "text"
      },
      "source": [
        "#Search for the Optimal Number of Principal Components\n",
        "Now lets conduct a few experiments by reducing the number of principal components PCA generates and evaluate the fraud detection results.<br>\n",
        "\n",
        "*We need the PCA based fraud detection solution to have enough error on the rare cases that it can meaningfully separate fraud cases from the normal ones. But thr error cannot be so low or so high for all transactions that are rare and normal transactions are virtually indistinguishable.***bold text***<br>\n",
        "\n",
        "After running the above code, we can see that we are able to catch 80% of the fraud with 75% precision.<br>\n",
        "*This is very impressive seeing that we did not use ANY LABELS!!!\n",
        "\n",
        "Using PCA, we calculated the reconstruction error for each of these 190,820 transactions. If we sort these transactions by highest reconstruction error (also referred to as anomaly score) in descending order and extract the top 350 transactions from the list, we can see that 264 of these transactions are fraudulent.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRLy7JiBt3wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code\n",
        "preds.sort_values(by= 'anomaly_score', ascending= False, inplace= True)\n",
        "cutoff= 350\n",
        "preds_top= preds[:cutoff]\n",
        "print('Precision: 'np.round(preds_top.anomaly_score[preds_top.truelabel==1].count()/cutoff, 2))\n",
        "print('Recall: ', np.round(preds_top.anomaly_score[preds_top.truelabel==1].count()/y_train.sum()/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp75b9sPCEgU",
        "colab_type": "text"
      },
      "source": [
        "#Sparse PCA Anomaly \n",
        "Lets try implementing the SparsePCA algorithm to our fraud detection dataset.<br>\n",
        "*Remember that Sparse PCA is very similar to the standard PCA, the only difference is that it is a less dense version of PCA.\n",
        "***bold text***\n",
        "\n",
        "We will need to specify the number of principal components we desire, but we must also set the *alpha parameter*, which control the degree of sparsity.\n",
        "\n",
        "We will experiment with different values for the principal components and the alpha parameters as we search for the optimal sparse PCA fraud detection solution.\n",
        "\n",
        "**Note that the normal PCA within sklearn used a ***fit_transform*** function to generate the principal components and an ***inverse_transform*** function to reconstruct the original dimensions from the principal components. <br>\n",
        "Using these two functions will allow us to calculate the reconstruction error between the original feature set and the reconstructed feature set derived from the PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkADSSO9eq81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sparse PCA\n",
        "from sklearn.decomposition import SparsePCA\n",
        "n_components= 27\n",
        "alpha= 0.0001\n",
        "random_state= 2018\n",
        "n_jobs= -1\n",
        "\n",
        "sparse_pca= SparsePCA(n_components= n_components, alpha= alpha,\n",
        "                      random_state= random_state, n_jobs= n_jobs)\n",
        "sparse_pca.fit(x_train.loc[:,:])\n",
        "x_train_sparse_pca= sparse_pca.transform(x_train)\n",
        "x_train_sparse_pca= pd.DataFrame(data= x_train_sparse_pca, index= x_train.index)\n",
        "\n",
        "scatterPlot(x_train_sparse_pca, y_train, 'SparsePCA')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsAFK6Rcf1wd",
        "colab_type": "text"
      },
      "source": [
        "Now lets generate the original dimensions from sparse PCA matrix by simple matrix mulriplication of the Sparse PCA matrix (with 190,820 samples and 27 dimensions) and the sparse PCA components  (a 27 x 30 matrix).<br>\n",
        "This creates a matrix that is the original size(a 190, 820 x 30 matrix).<br>\n",
        "We also need to ***add the mean*** of the original feature to this new matrix, but then we are done.<br>\n",
        "\n",
        "*From this newly derived inverse matrix, we can calculate the reconstruction errors (anomaly scores) as we did with normal PCA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbh3j3zQg5W7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#applying the inverse pca function\n",
        "x_train_sparse_pca_inverse= np.array(x_train_sparse_pca).dot(sparse_pca.compenents_) + np.array(\n",
        "    x_train.mean(axis = 0))\n",
        "x_train_sparse_pca_inverse= pd.DataFrame(data= x_train_sparse_pca_inverse, index= x_train.index)\n",
        "anomaly_scores_sparse_pca= anomaly_scores(x_train, x_train_sparse_pca_inverse)\n",
        "preds= plot_results(y_train, anomaly_scores_sparse_pca, True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsA6Kz1Gh1YU",
        "colab_type": "text"
      },
      "source": [
        "#Kernel PCA Anomaly Detection\n",
        "In this example, we will create a fraud detection system using the kernel pca algorithm which is a** non-linear form of PCA** and is useful for fraud transactions that are not linearly seperable from the non-fraid transactions.\n",
        "\n",
        "We need tp specify the number of components we would like to generate, the kernel (we will use RBF kernelas we did in the previous notebook), and the gamma (which is set to 1/n_features by defualt, so 1/30 in our case).<br>\n",
        "We will also need to set the fit_inverse_transform to True to apply built-in inverse_transform function provided by sklearn\n",
        "\n",
        "Finally, because kernel pca is so expensive to train with, we will train on just the first two thousand samples in the transactions dataset. **(this is not ideal, but  it is necessary in order to perform experiments quickly)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep71ZhT-FjEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Kernel pca\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "n_components = 27\n",
        "kernel= 'rbf'\n",
        "gamma= None\n",
        "fit_inverse_transform= True\n",
        "random_state= 2018\n",
        "n_jobs= 1\n",
        "\n",
        "kernel_pca= KernelPCA(n_components= n_components, kernel= kernel,\n",
        "                      gamma= gamma, fit_inverse_transform= fit_inverse_transform,\n",
        "                      random_state= random_state, n_jobs= n_jobs)\n",
        "kernel_pca.fit(x_train.loc[:2000])\n",
        "\n",
        "x_train_kernelpca= kernel_pca.transform(x_train)\n",
        "x_train_kernelpca= pd.DataFrame(data= x_train_kernelpca, index= x_train.index)\n",
        "\n",
        "x_train_kernelpca_inverse= kernel_pca.inverse_transform(x_train_kernelpca)\n",
        "x_train_kernelpca_inverse= pd.DataFrame(data= x_train_kernelpca_inverse, index= x_train.index)\n",
        "\n",
        "scatterPlot(x_train_kernelpca, y_train, 'Kernel PCA')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8rI6kV0G9nH",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the results are far worse than those using the normal PCA or Sparse PC.<br>\n",
        "Whileit was worth experimenting with Kernel PCA, we will not use this solution for fraud detection given that we have better performing solutions from other algorithms.\n",
        "\n",
        "*We will not create any fraud detection systems using SVD seeing that it is very similar to normal PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gdsF4sdIrBe",
        "colab_type": "text"
      },
      "source": [
        "#Gaussian Random Projection Anomaly Detection\n",
        "Remember here that we can set either the number of components we want or the eps parameter, which controls quality of the embedding derived based on Johnson-Lindenstrauss lemma.<br>\n",
        "We will choose to explicitly set the number of components.\n",
        "(Gaussian Random projection trains very quickly, so we can train on the entire training set)<br>\n",
        "*As with Sparse pca, we willneed to derive our own inverse_transform function because none is provided by sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4wEeYZyKI2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Gaussian Random Projection\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "\n",
        "n_components= 27\n",
        "eps= None\n",
        "random_state= 2018\n",
        "\n",
        "grp= GaussianRandomProjection(n_components= n_components, eps= eps, random_state= random_state)\n",
        "\n",
        "x_train_grp= grp.fit_transform(x_train)\n",
        "x_train_grp= pd.DataFrame(data= x_train_grp, index = x_train.index)\n",
        "\n",
        "scatterPlot(x_train_grp, y_train, 'Gaussian Random Projection')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rps76mUlKzw_",
        "colab_type": "text"
      },
      "source": [
        "As we can see these results are poor....so we won't be using this algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfEL1TXgK60S",
        "colab_type": "text"
      },
      "source": [
        "## Sparse Random Projection Anomaly Detection\n",
        "Here, we will designate the number of components we want (instead of setting the eps parameter). And like with Gaussian random projection, we will use our own inverse_transform function to create the original dimensions from the sparse random projection derived components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDtxvmM9PRQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sparse Random Projection\n",
        "from sklearn.random_projection import SparseRandomProjection\n",
        "\n",
        "n_components= 27\n",
        "density = 'auto'\n",
        "eps= .01\n",
        "dense_output= True\n",
        "random_state= 2018\n",
        "\n",
        "srp= SparseRandomProjection(n_components= n_components, density= density, eps= eps,\n",
        "                            dense_output= dense_output, random_state= random_state)\n",
        "\n",
        "x_train_srp= srp.fit_transform(x_train)\n",
        "x_train_srp= pd.DataFrame(data= x_train_srp, index = x_train.index)\n",
        "\n",
        "scatterPlot(x_train_srp, y_train, 'Sparse Random Projection')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_xASuu-QIYh",
        "colab_type": "text"
      },
      "source": [
        "As with Gaussian random projection, these results are poor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vawpepDJd395",
        "colab_type": "text"
      },
      "source": [
        "#Nonlinear Anomaly Detection\n",
        "At this point we can see that PCAis the best solution thus far in creating a fraud detection system using unsupervised learning algorithms.\n",
        "\n",
        "Now, we can turn to nonlinear dimensionality reduction techniques, BUT the open source versions of these algorithms run very slowly and are not viable for fast fraud detection.\n",
        "For these reasons, we will skip this and go directly to \"nonsdistance-based\" dimensionality reduction methods:\n",
        "- dictionary learning and independent component analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxEcEdKReuOJ",
        "colab_type": "text"
      },
      "source": [
        "#Dictionary Learning Anomaly Detection\n",
        "In dictionary learning, the algorithm learns the sparse representation of the original data.\n",
        "\n",
        "For anolay detection, we want to learn the \"undercomplete\" so that the vectors in the dictionary are fewer in number than the original dimensions.\n",
        "\n",
        "In our case, we will generate 28 vectors (or components) to learn the dictionary we will feed in 10 batches, where each batch has 200 samples.<br>\n",
        "*We will need to use our own inverse_transform function as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxE0I9GBfi1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
        "\n",
        "n_components= 28\n",
        "alpha= 1\n",
        "batch_size= 200\n",
        "n_iter= 10\n",
        "random_state= 2018\n",
        "\n",
        "mbl= MiniBatchDictionaryLearning(n_component= n_components, alpha= alpha,\n",
        "                                 batch_size= batch_size, n_iter= n_iter, random_state= random_state)\n",
        "mb.fit(x_train)\n",
        "\n",
        "x_train_mbl= mbl.fit_transform(x_train)\n",
        "x_train_mbl= pd.DataFrame(data= x_train_mbl, index = x_train.index)\n",
        "\n",
        "scatterPlot(x_train_mbl, y_train, 'MiniBatch Dictionary Learning')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrl2OnQagfSZ",
        "colab_type": "text"
      },
      "source": [
        "These results are better than Kernel PCA, Gaussian Random Projection and sparse random projection BUT are know match for normal PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp0kdXVOhaEN",
        "colab_type": "text"
      },
      "source": [
        "#ICA Anomaly Detection\n",
        "Last use ICA to design our final fraud detection solution.<br>\n",
        "We need to first specify the number of components, which we will set to 27. (using sklearn inverse_transform function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pPnaCKUhuFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ICA\n",
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "n_components= 27\n",
        "algorithm= 'parallel'\n",
        "whiten= True\n",
        "max_iter= 200\n",
        "random_state= 2018\n",
        "\n",
        "fast_ica= FastICA(n_components= n_components, algorithm= algorithm,\n",
        "                  whiten= whiten, max_iter= max_iter, random_state)\n",
        "\n",
        "x_train_fastica= fast_ica.fit_transform(x_train)\n",
        "x_train_fatstica= pd.DataFrame(data= x_train_fastica, index= x_train.index)\n",
        "\n",
        "x_train_fastica_inverse= fast_ica.inverse_transform(x_train_fastica)\n",
        "x_train_fastica_invers= pd.DataFrame(data= x_train_fastica_inverse, index= x_train.index)\n",
        "\n",
        "scatterPlot(x_train_fast_ica, y_train, 'Independent Component Analysis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBFNW6p8jAJ8",
        "colab_type": "text"
      },
      "source": [
        "As we can see these results match those of normal PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Szr2lQujFdM",
        "colab_type": "text"
      },
      "source": [
        "#Fraud Detection on Test \n",
        "Now to evaluate our fraud detection solutions, lets apply them to never before seen test set.<br>\n",
        "*We will do this for the top three solutions:\n",
        "- PCA\n",
        "- ICA\n",
        "- Dictioary Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AjBeObppX7L",
        "colab_type": "text"
      },
      "source": [
        "#PCA (test set)\n",
        "We will use the PCA embedding that the PCA algorithm learned from the training set and use this to transform the test set.<br>\n",
        "We will then use the sklearn inverse-transform function to recreate the original dimensions from the principal components matrix of the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8p7amo6p_ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PCA on Tets Set\n",
        "x_test_pca= pca.transform(x_test)\n",
        "x_test_pca= pd.DataFrame(data= x_test_pca, index= x_test.index)\n",
        "\n",
        "x_test_pca_inverse= pca.invserse_transform(x_test_pca)\n",
        "x_test_pca_inverse= pd.DataFrame(data= x_test_pca_inverse, index= x_test.index)\n",
        "\n",
        "scatterPlot(x_test_pca, y_test, 'PCA')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loixn18Fq99T",
        "colab_type": "text"
      },
      "source": [
        "T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFc1Oo16q-RF",
        "colab_type": "text"
      },
      "source": [
        "These are impressive results. We are able to catch 80% of the known fraud in the tets set with an 80% precision-- all without using any labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-ytKcR7rMLJ",
        "colab_type": "text"
      },
      "source": [
        "#ICA Anomaly Detection (On The Test Set)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7_nX_F-rSDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Independent Component Analysis\n",
        "\n",
        "x_test_fastica= fast_ica.transform(x_test)\n",
        "x_test_fastica= pd.DataFrame(data= x_test_fastica, index  x_test.index)\n",
        "\n",
        "x_test_fastica_inverse= fast_ica.inverse_transform(x_test_fast_ica)\n",
        "x_test_fastica_inverse= pd.DataFrame(data= x_test_fastica_inverse, index = x_test.index)\n",
        "\n",
        "scatterPlot(x_test_fast_ica, y_train, 'Independent Component Analysis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F5g8b0XsIAn",
        "colab_type": "text"
      },
      "source": [
        "The results are the same as normal PCA and thus quite impressive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6laNIkP0sN1Z",
        "colab_type": "text"
      },
      "source": [
        "#Dictionary Learning (On test Set)\n",
        "Lets now use dictionary learing which did not perform as well as normal PCA and ICA, but it's still worth a shot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyVHsJSisgSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dictionary Learning on test set\n",
        "x_test_mbl= mbl.transform(x_test)\n",
        "x_test_mbl= pd.DataFrame(data= x_test_mbl, index= x_test.index)\n",
        "\n",
        "scatterPlot(x_test_mbl, y_train, 'Mini- batch Dictionary Learning')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOoMiqvItEj1",
        "colab_type": "text"
      },
      "source": [
        "Whie the results are not terrible- we can catch 80% of the fraud with 20% precision- they fall far short of the results from normal PCA and ICA."
      ]
    }
  ]
}